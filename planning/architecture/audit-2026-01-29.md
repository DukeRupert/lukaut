# Lukaut Architecture Audit
**Date**: 2026-01-29
**Scope**: Full codebase - Pre-alpha launch review
**Reviewer**: Architecture Review Agent

## Executive Summary

The Lukaut codebase demonstrates **solid architectural foundations** with clear separation of concerns and good adherence to layered architecture principles. The project is well-positioned for alpha launch with a single client. The domain layer is clean and testable, services properly orchestrate business logic, and infrastructure concerns are appropriately abstracted.

**Key Strengths:**
- Clean domain layer with zero infrastructure dependencies
- Well-defined service interfaces enabling testability
- Proper AI provider abstraction with mock implementation
- Comprehensive error handling with domain-specific error types
- Good state machine implementation for inspection workflow
- Storage abstraction supporting local dev and production R2

**Primary Concerns for Alpha:**
1. ~~**Handler-Repository coupling** - Handlers bypass services for job enqueueing (medium priority)~~ ✅ **RESOLVED 2026-01-29**
2. **Multi-tenancy preparation** - Current user_id scoping is good but lacks explicit tenant context (low priority for alpha, high for SaaS scale)
3. ~~**Missing service layer coverage** - Some handlers fetch data directly from repository queries (medium priority)~~ ✅ **PARTIALLY RESOLVED 2026-01-30** (regulation.go complete; dashboard.go and admin.go remain low priority)

**Recommendation:** The architecture is **sound for alpha launch**. Multi-tenancy preparation can be deferred until validating product-market fit.

---

## Current State

### Package Map

**Core Business Layers:**
- `/internal/domain/` - Pure business types and logic (Inspection, Violation, User, Client, Report, Image, Token)
  - **Zero infrastructure dependencies** ✓
  - Includes state machines (InspectionStatus, ViolationStatus)
  - Domain error types (Error, ValidationError)
  - Service parameter types (CreateInspectionParams, etc.)

- `/internal/service/` - Business logic orchestration
  - `inspection.go` - Inspection CRUD and state transitions (677 lines)
  - `violation.go` - Violation management (555 lines)
  - `user.go` - Authentication, registration, profile (1,229 lines)
  - `client.go` - Client management
  - `image.go` - Image upload, storage integration
  - `report.go` - Report generation coordination
  - `thumbnail.go` - Image processing

- `/internal/repository/` - Data access layer (sqlc-generated)
  - `***.sql.go` - Type-safe database queries
  - `models.go` - Database model types (sql.Null* types)
  - `db.go` - Query struct and transaction support

**Infrastructure:**
- `/internal/handler/` - HTTP request handlers (thin, delegate to services)
- `/internal/middleware/` - Auth, subscription checks, metrics
- `/internal/ai/` - AI provider abstraction
  - `ai.go` - AIProvider interface
  - `anthropic/` - Claude API implementation
  - `mock/` - Test double
- `/internal/storage/` - File storage abstraction (local/R2)
- `/internal/email/` - Email service (SMTP/Postmark)
- `/internal/billing/` - Stripe integration
- `/internal/worker/` - Background job processing
- `/internal/jobs/` - Job handler implementations
- `/internal/report/` - PDF/DOCX generation (WeasyPrint, Pandoc)
- `/internal/metrics/` - Prometheus metrics
- `/internal/session/` - Session management (not a package, see note below)
- `/internal/auth/` - Auth context helpers
- `/internal/csrf/` - CSRF protection
- `/internal/invite/` - Invite code validation (MVP gating)

**Presentation:**
- `/internal/templ/` - templ components, layouts, pages (server-side rendering)

### Dependency Analysis

**Dependency Flow:**
```
handlers → services → repositories → database
    ↓          ↓
  domain ← domain
    ↓
infrastructure (ai, storage, email, billing)
```

**Key Observations:**

1. **Domain Isolation: EXCELLENT**
   - Domain package imports: `database/sql`, `errors`, `fmt`, `time`, `github.com/google/uuid`
   - The `database/sql` import is **acceptable** - used only for helper functions converting between sql.Null* types and domain types (e.g., `NullStringValue`, `ToNullString`)
   - Zero framework dependencies
   - Zero HTTP dependencies
   - Zero database driver dependencies
   - Can be tested in complete isolation ✓

2. **Service Layer: GOOD**
   - Services depend only on: domain, repository, logger
   - Services define interfaces (InspectionService, UserService, etc.)
   - Proper error translation: database errors → domain errors
   - State transitions validated through domain methods

3. **Handler Layer: MOSTLY GOOD with COUPLING ISSUE**
   - Handlers import both `service` and `repository` packages
   - **Issue**: Handlers use `h.queries` directly for job enqueueing:
     ```go
     worker.EnqueueAnalyzeInspection(r.Context(), h.queries, id, user.ID)
     ```
   - This bypasses the service layer and creates handler→repository coupling
   - **Location**: `/internal/handler/inspection.go:487`

4. **AI Abstraction: EXCELLENT**
   - Clean interface definition in `/internal/ai/ai.go`
   - Provider-agnostic types (AnalyzeImageParams, AnalysisResult)
   - Mock implementation for testing
   - Anthropic-specific details encapsulated in `anthropic/` subpackage

5. **Background Jobs: GOOD PATTERN**
   - Job handlers in `/internal/jobs/` properly orchestrate services
   - Worker package manages job queue infrastructure
   - **Issue**: Job enqueueing exposed as package-level functions requiring `*repository.Queries`

**No Circular Dependencies Detected** ✓

---

## Findings

### What's Working Well

1. **Domain-Driven State Management**
   - Inspection lifecycle enforced through `InspectionStatus.CanTransitionTo()`
   - State transitions validated in domain, enforced by services
   - Clear separation between draft/analyzing/review/completed states
   - Makes invalid state transitions impossible at compile time

2. **Service Interfaces for Testability**
   - All major services define interfaces (InspectionService, UserService, etc.)
   - Enables mocking for handler tests
   - Clear contracts between layers

3. **Error Handling Strategy**
   - Domain-specific error types (`domain.Error`) with error codes
   - Error translation at service boundaries (sql.ErrNoRows → domain.ENOTFOUND)
   - Structured error context (code, operation, message, wrapped error)
   - HTTP handlers map error codes to status codes cleanly

4. **AI Provider Abstraction**
   - `AIProvider` interface enables swapping implementations
   - Mock provider for testing without API calls
   - Usage tracking built into provider layer
   - Confidence and severity types shared between AI and domain layers

5. **Storage Abstraction**
   - `Storage` interface supports local dev (filesystem) and production (R2)
   - Configuration-driven provider selection
   - Clean separation between storage infrastructure and domain concerns

6. **Proper Repository Pattern**
   - sqlc generates type-safe queries
   - All queries include user_id for data isolation (good foundation for multi-tenancy)
   - Transaction support via `WithTx()`
   - Separation between repository.User (sql.Null* types) and domain.User (clean types)

7. **Background Job Architecture**
   - Database-backed queue with SKIP LOCKED for concurrency
   - Retry logic with exponential backoff
   - Job-specific handlers implementing common interface
   - Metrics for monitoring job success/failure

### Areas of Concern

---

#### Handler-Repository Coupling ✅ RESOLVED

**Severity**: Medium
**Status**: ✅ **RESOLVED 2026-01-29**

**Original issue**: Handlers imported `repository` package and used `h.queries` directly for job enqueueing, bypassing the service layer.

**Resolution implemented:**

1. **Created `JobEnqueuer` interface** in `internal/worker/enqueue.go`:
   ```go
   type JobEnqueuer interface {
       EnqueueAnalyzeInspection(ctx context.Context, inspectionID, userID uuid.UUID, opts ...EnqueueOption) (repository.Job, error)
       EnqueueGenerateReport(ctx context.Context, inspectionID, userID uuid.UUID, format, recipientEmail string, opts ...EnqueueOption) (repository.Job, error)
   }
   ```

2. **Extended InspectionService** with job-related methods:
   - `TriggerAnalysis(ctx, inspectionID, userID)` - validates eligibility and enqueues analysis
   - `HasPendingAnalysisJob(ctx, inspectionID)` - checks for pending jobs

3. **Extended ReportService** with CRUD and job methods:
   - `GetByID(ctx, id, userID)` - fetches report with authorization
   - `ListByInspection(ctx, inspectionID, userID)` - lists reports for inspection
   - `TriggerGeneration(ctx, inspectionID, userID, format, email)` - enqueues report generation

4. **Refactored handlers** to use service methods:
   - `InspectionHandler` - removed `queries`, uses `inspectionService.TriggerAnalysis()`
   - `ImageHandler` - removed `queries`, uses `inspectionService.HasPendingAnalysisJob()` and `TriggerAnalysis()`
   - `ReportHandler` - replaced `queries` with `reportService`, deleted ~125 lines of duplicate `aggregateReportData()`

5. **Created adapter in main.go** to bridge worker.JobEnqueuer (with variadic options) to service.JobEnqueuer (simplified interface) to avoid circular imports.

**Files modified:**
- `internal/worker/enqueue.go` - Added JobEnqueuer interface + implementation
- `internal/service/inspection.go` - Added TriggerAnalysis, HasPendingAnalysisJob, jobEnqueuer field
- `internal/service/report.go` - Added GetByID, ListByInspection, TriggerGeneration, jobEnqueuer field
- `internal/handler/inspection.go` - Removed queries, added reportService
- `internal/handler/inspection_new.go` - Updated to use service methods
- `internal/handler/image.go` - Removed queries, use service methods
- `internal/handler/report.go` - Replaced queries with reportService, deleted duplicate code
- `cmd/server/main.go` - Added serviceJobEnqueuer adapter, updated wiring

**Commit:** `8aaa878 refactor: decouple handlers from direct repository access`

---

#### Database Session Management in /internal/session

**Severity**: Low
**Current state**: No dedicated `/internal/session/` package found - session logic appears to be in `UserService`

**Observation:** Session creation, validation, and cleanup is handled in `/internal/service/user.go` (GetBySessionToken, Login, Logout). This is actually a **reasonable approach** for a simple session strategy. Sessions are treated as part of user authentication, not separate infrastructure.

**Risk if unchanged:** None for current scope. This is a non-issue.

**Verdict:** ✓ Current approach is fine for alpha.

---

#### Missing Service Layer Coverage for Data Fetching ✅ PARTIALLY RESOLVED

**Severity**: Medium
**Status**: ✅ **regulation.go RESOLVED 2026-01-30** | dashboard.go and admin.go remain (low priority)

**Original issue**: Some handlers fetched data directly from repository for read-only operations.

**Handlers audited:**
- **regulation.go** - ✅ RESOLVED - Had 12 direct repo calls including authorization logic and write operations
- **dashboard.go** - Deferred (read-only stats aggregation, low risk)
- **admin.go** - Deferred (admin-only, read-only, low risk)

**Resolution for regulation.go (2026-01-30):**

1. **Created `domain/regulation.go`** with domain types:
   - `Regulation`, `RegulationSummary`, `RegulationSearchResult`
   - `LinkRegulationParams`, `UnlinkRegulationParams`

2. **Created `service/regulation.go`** with `RegulationService` interface:
   ```go
   type RegulationService interface {
       GetByID(ctx, id) (*domain.Regulation, error)
       Search(ctx, query, limit, offset) (*domain.RegulationSearchResult, error)
       Browse(ctx, category, limit, offset) (*domain.RegulationSearchResult, error)
       ListCategories(ctx) ([]string, error)
       LinkToViolation(ctx, params) error      // idempotent
       UnlinkFromViolation(ctx, params) error  // idempotent
       IsLinkedToViolation(ctx, violationID, regulationID) (bool, error)
   }
   ```

3. **Refactored handlers** to use service:
   - `RegulationHandler` now depends on `RegulationService` + `ViolationService`
   - Removed `repo` field entirely
   - Authorization checks moved to service layer
   - Link/unlink operations are now idempotent

4. **Files modified:**
   - `internal/domain/regulation.go` - Created
   - `internal/service/regulation.go` - Created
   - `internal/handler/regulation.go` - Refactored
   - `internal/handler/regulation_new.go` - Refactored
   - `cmd/server/main.go` - Added service wiring

**Remaining work (low priority):**
- **dashboard.go** - 5 direct repo calls for stats (read-only aggregation)
- **admin.go** - 5 direct repo calls for admin panel (admin-only)

These are deferred because they are read-only operations with no business logic at risk. Can be addressed when scaling beyond alpha.

---

#### Multi-tenancy Preparation

**Severity**: Low (for alpha), High (for SaaS scaling)
**Current state**: All data is scoped by `user_id` in queries. No explicit tenant concept.

**What's already good:**
- All queries include `WHERE user_id = $1` or `WHERE id = $1 AND user_id = $2`
- Data isolation at user level works
- No cross-user data leakage risk

**What's missing for true multi-tenancy:**
- No `tenant_id` or `organization_id` concept
- Users cannot share resources within a team/company
- No role-based access control (owner, admin, member)
- No tenant-level settings or limits

**Example query from `/home/dukerupert/Repos/lukaut/sqlc/queries/inspections.sql`:**
```sql
WHERE id = $1 AND user_id = $2;
```

**Future multi-tenancy pattern would be:**
```sql
WHERE id = $1 AND tenant_id = (SELECT tenant_id FROM users WHERE id = $2);
```

**Risk if unchanged:**
- When you onboard a company with multiple inspectors, they can't share inspections
- Can't have admin users managing team accounts
- Harder to add teams later (require migration to add tenant_id to all tables)
- Billing will be per-user instead of per-organization

**Recommended approach** (for future, NOT alpha):

1. **Introduce tenant concept:**
   - Add `tenants` table (id, name, subscription_status, stripe_customer_id)
   - Add `tenant_id` to `users` table
   - Add `tenant_id` to all domain tables (inspections, clients, violations, images, reports)

2. **Add team membership:**
   - Add `team_members` table (tenant_id, user_id, role)
   - Roles: owner, admin, inspector

3. **Scope queries by tenant:**
   - Change `WHERE user_id = $1` to `WHERE tenant_id = (SELECT tenant_id FROM users WHERE id = $1)`
   - For user-specific data (sessions, tokens), keep user_id scoping

4. **Authorization service:**
   - Create `internal/authz/` package for permission checks
   - Example: `CanEditInspection(userID, inspectionID uuid.UUID) bool`

**Multi-tenancy readiness assessment:**

**Green flags (makes future migration easier):**
- All queries already scoped (by user_id, not global)
- UUID primary keys (no sequential IDs that could leak tenant size)
- Service layer exists (good place to add tenant authorization)
- Repository pattern (can update queries in one place)

**Yellow flags (will need refactoring):**
- No tenant_id in schema yet
- Authentication middleware assumes single-user ownership
- Billing tied to individual users, not organizations

**Expected effort for tenant migration**: Large (2-4 weeks)
- Schema migration to add tenant_id everywhere
- Update all queries to scope by tenant instead of user
- Add team membership and role system
- Update authorization logic in services
- Migrate existing data to single-user tenants
- Update billing logic to be tenant-based

**Recommended prioritization for alpha:**
- **DEFER** until you have 3+ clients asking for team features
- Current user_id scoping is sufficient for single-inspector accounts
- Focus on product validation, not premature multi-tenancy

**When to address:**
- If a client says "we have 5 inspectors who need to share inspections"
- If billing model changes to per-company instead of per-user
- If you need admin users to manage team accounts

---

#### sql.Null* Types in Domain Package

**Severity**: Low
**Current state**: Domain package imports `database/sql` for helper functions

**Code in `/internal/domain/user.go`:**
```go
import (
    "database/sql"
    // ...
)

// NullStringValue safely extracts a string from sql.NullString.
func NullStringValue(ns sql.NullString) string {
    if ns.Valid {
        return ns.String
    }
    return ""
}
```

**Why this exists:**
Services need to convert between repository models (sql.Null* types) and domain models (clean types). These helper functions live in domain package for convenience.

**Risk if unchanged:**
- Couples domain to database/sql package (minor philosophical violation)
- Not actually a practical problem - these are pure functions with no side effects

**Alternative approaches:**

1. **Move helpers to service package** (recommended if you care about domain purity)
   ```go
   // In internal/service/conversion.go
   func nullStringValue(ns sql.NullString) string { ... }
   ```

2. **Create adapter package** (overkill for current size)
   ```go
   // In internal/adapter/repository.go
   func ToDomainUser(repoUser repository.User) domain.User { ... }
   ```

3. **Keep as-is** (pragmatic choice)
   - Minimal coupling
   - No behavior, just type conversion
   - Doesn't impact testability

**Recommended approach:** Keep as-is for alpha. The coupling is minimal and these functions are pure transformations. If domain package grows to 5000+ lines, consider extracting to adapter layer.

**Expected value of change:** Low. This is architectural purity, not a practical issue.

**Effort estimate**: Small (30 minutes to move functions, update imports)

---

#### Report Generation Relies on External Commands

**Severity**: Low
**Current state**: Report generation uses `weasyprint` (PDF) and `pandoc` (DOCX) via os/exec

**Code in `/internal/report/converter.go`:**
```go
func (c *WeasyPrintConverter) Convert(ctx context.Context, html []byte, w io.Writer) error {
    // Create temp directory for input/output files
    tmpDir, err := os.MkdirTemp("", "report-pdf-*")
    // ...
    cmd := exec.CommandContext(ctx, c.Command, inputPath, outputPath)
    // ...
}
```

**Why this approach:**
- WeasyPrint and Pandoc are mature, battle-tested tools
- Go PDF generation libraries are limited for complex layouts
- HTML→PDF via external tool is common pattern

**Risks:**
- External dependencies must be installed on server (Dockerfile must include weasyprint, pandoc)
- Subprocess execution has security implications (input sanitization critical)
- Temp file cleanup could leak disk space if errors occur
- Harder to test (requires external tools installed)

**Mitigations already in place:**
- Uses `exec.CommandContext` with timeout (prevents hanging)
- Temp directory creation (isolates files)
- Context cancellation support

**Recommended improvements** (low priority):
1. Add cleanup defer in error paths (ensure temp files deleted)
2. Add integration tests that skip if tools not installed
3. Document installation requirements clearly in deployment docs
4. Consider timeout configuration (very large reports could hit timeout)

**Expected value of addressing**: Low. This approach is pragmatic and works. Alternative pure-Go solutions would be significantly more effort for marginal benefit.

**Effort estimate**: Small (add defer cleanup, better error messages, docs)

---

#### No Rate Limiting on Job Enqueueing ✅ RESOLVED

**Severity**: Low (for alpha with 1 client), Medium (for multi-client SaaS)
**Status**: ✅ **RESOLVED 2026-01-30**

**Original issue**: Handlers could enqueue jobs without rate limiting, risking Anthropic API quota drain or high costs.

**Resolution implemented:**

1. **Created tiered quota system** with subscription-based limits:
   - Free tier: 3 analysis jobs/month, 2 report jobs/month
   - Starter tier: Unlimited (flag-based, easy to add limits later)
   - Professional tier: Unlimited

2. **Created `domain/quota.go`** with quota types:
   ```go
   type QuotaType string // "analysis" or "report"
   type TierQuota struct {
       AnalysisPerMonth, ReportsPerMonth int
       UnlimitedAnalysis, UnlimitedReports bool
   }
   var TierQuotas = map[SubscriptionTier]TierQuota{...}
   ```

3. **Created `service/quota.go`** with QuotaService:
   ```go
   type QuotaService interface {
       GetUsage(ctx, userID, tier) (*domain.QuotaUsage, error)
       CheckAnalysisQuota(ctx, userID, tier) error  // returns nil or QuotaExceeded
       CheckReportQuota(ctx, userID, tier) error
   }
   ```

4. **Added `QuotaExceeded` error** to `domain/errors.go`:
   - Returns `ERATELIMIT` error code (maps to HTTP 429)
   - Message: "Monthly analysis quota exceeded (3/3). Upgrade your plan for more."

5. **Modified services** to check quota before enqueueing:
   - `InspectionService.TriggerAnalysis()` - calls `quotaService.CheckAnalysisQuota()`
   - `ReportService.TriggerGeneration()` - calls `quotaService.CheckReportQuota()`

6. **Added SQL query** `CountCompletedJobsByUserAndType` to count jobs by user within date range.

**Behavior:**
| User State | Tier Used | Limits |
|------------|-----------|--------|
| Active subscription (starter/professional) | Paid tier | Unlimited |
| Trialing | Paid tier | Unlimited |
| Inactive/no subscription | Free | 3 analysis, 2 reports/month |

**Files modified:**
- `internal/domain/user.go` - Added `SubscriptionTierFree` constant
- `internal/domain/quota.go` - Created quota types
- `internal/domain/errors.go` - Added `QuotaExceeded()` constructor
- `sqlc/queries/jobs.sql` - Added `CountCompletedJobsByUserAndType` query
- `internal/service/quota.go` - Created QuotaService
- `internal/service/inspection.go` - Added quota check in TriggerAnalysis
- `internal/service/report.go` - Added quota check in TriggerGeneration
- `cmd/server/main.go` - Wired up QuotaService

---

## Multi-tenancy Readiness Assessment

### Current Architecture

**Data Isolation: User-scoped (Good foundation)**
- All domain tables include `user_id` foreign key
- All queries filter by `user_id`
- Authorization enforced at service layer (GetByID checks user ownership)

**Example from inspection service:**
```go
func (s *inspectionService) GetByID(ctx context.Context, id, userID uuid.UUID) (*domain.Inspection, error) {
    row, err := s.queries.GetInspectionWithClientByIDAndUserID(ctx, repository.GetInspectionWithClientByIDAndUserIDParams{
        ID:     id,
        UserID: userID,
    })
    // ...
}
```

This pattern is **correct** and prevents cross-user data leakage.

### What's Missing for Multi-tenancy

1. **No Tenant Concept**
   - Users cannot belong to organizations
   - No shared access to resources within a team
   - Billing is per-user, not per-organization

2. **No Role-Based Access Control**
   - Binary ownership (you own it or you don't)
   - No concept of admin vs member vs viewer
   - No delegated permissions

3. **No Tenant-Level Settings**
   - User preferences are individual
   - No company-wide settings (branding, compliance requirements, etc.)

4. **Schema Changes Required**
   - Need to add `tenant_id` to all domain tables
   - Need `tenants` table
   - Need `team_memberships` table with roles

### Migration Path to Multi-tenancy

**Phase 1: Introduce Tenants (No breaking changes)**
1. Add `tenants` table
2. Add `tenant_id` to `users` table (nullable initially)
3. Migrate existing users to single-user tenants
4. Make `tenant_id` required (migration backfills)

**Phase 2: Scope Resources by Tenant**
1. Add `tenant_id` to inspections, clients, images, violations, reports
2. Add foreign key constraints
3. Update queries: `WHERE user_id = $1` → `WHERE tenant_id = (SELECT tenant_id FROM users WHERE id = $1)`

**Phase 3: Add Team Features**
1. Add `team_memberships` table (tenant_id, user_id, role)
2. Add authorization service for role checks
3. Update middleware to load tenant context
4. Update services to check permissions (not just ownership)

**Phase 4: Tenant-Level Billing**
1. Move Stripe customer from users to tenants
2. Update subscription logic
3. Add usage tracking per tenant

### Assessment

**Ready for single-client alpha?** ✓ Yes
**Ready for 10 single-user clients?** ✓ Yes (current user_id scoping works)
**Ready for team accounts?** ✗ No (requires tenant migration)

**Recommendation:** Ship alpha with current architecture. Plan tenant migration when:
- You have 3+ prospects asking for team features
- You're ready to change billing model to per-organization
- You have validated product-market fit with individual users

**Estimated effort for full multi-tenancy:** 3-4 weeks
- Schema migration: 1 week
- Query updates: 1 week
- Authorization service: 1 week
- Testing and edge cases: 1 week

---

## Recommended Prioritization

### Critical for Alpha Launch

**Nothing critical found.** The architecture is sound for alpha.

### High Priority (Before Scaling to 5+ Clients)

1. ~~**Fix Handler-Repository Coupling**~~ ✅ **COMPLETED 2026-01-29**
   - Created JobEnqueuer interface and service methods
   - Refactored all handlers to use services
   - See "Handler-Repository Coupling" finding above for details

2. ~~**Audit and Complete Service Layer Coverage**~~ ✅ **PARTIALLY COMPLETED 2026-01-30**
   - **regulation.go** - DONE: Created RegulationService, moved authorization and business logic
   - **dashboard.go** - Deferred (read-only stats, low risk)
   - **admin.go** - Deferred (admin-only, read-only, low risk)
   - See "Missing Service Layer Coverage" finding above for details

### Medium Priority (Before 10+ Clients)

3. ~~**Add Rate Limiting on Job Enqueueing**~~ ✅ **COMPLETED 2026-01-30**
   - Implemented tiered quota system (free: 3 analysis, 2 reports/month; paid: unlimited)
   - QuotaService checks limits before job enqueueing
   - See "No Rate Limiting on Job Enqueueing" finding above for details

4. **Report Generation Hardening** [1 day]
   - Ensure temp file cleanup in all error paths
   - Add configurable timeouts
   - Better error messages for missing tools
   - Integration tests with tool checks
   - Why: Prevent disk space leaks, better debugging

### Low Priority (Defer Until Needed)

5. **Multi-tenancy Migration** [3-4 weeks]
   - Only when clients request team features
   - See detailed migration path above
   - Why: Don't build features before validating need

6. **Extract sql.Null* Helpers from Domain** [30 minutes]
   - Move to service or adapter package
   - Purely for architectural purity
   - Why: Minimal practical benefit, nice-to-have

---

## Appendix

### Dependency Graph (Simplified)

```
                   ┌─────────────┐
                   │   domain    │ (pure business logic)
                   └─────────────┘
                          ▲
                          │
         ┌────────────────┼────────────────┐
         │                │                │
    ┌─────────┐      ┌─────────┐     ┌─────────┐
    │ service │      │   ai    │     │ storage │
    └─────────┘      └─────────┘     └─────────┘
         ▲                ▲                ▲
         │                │                │
    ┌─────────┐      ┌──────────┐    ┌────────┐
    │ handler │      │ anthropic│    │   r2   │
    └─────────┘      │   mock   │    │  local │
         ▲           └──────────┘    └────────┘
         │
    ┌─────────┐
    │  HTTP   │
    └─────────┘
```

### File Statistics

**Total Go files in /internal:** ~120 files

**Lines of code by package (estimated):**
- domain: ~1,500 lines (pure business logic)
- service: ~5,000 lines (orchestration)
- handler: ~4,000 lines (HTTP)
- repository: ~2,500 lines (generated)
- jobs: ~500 lines (background processing)
- ai: ~800 lines (provider abstraction + implementation)
- Other infrastructure: ~3,000 lines

**Total internal codebase:** ~17,000 lines

### Test Coverage Notes

**Domain tests found:**
- `/internal/domain/inspection_test.go` (state machine tests)
- `/internal/domain/violation_test.go`

**Service tests found:**
- Some services lack tests (noted in audit)

**Handler tests found:**
- `/internal/handler/auth_test.go`
- `/internal/middleware/auth_test.go`

**Integration tests found:**
- `/internal/handler/verify_email_integration_test.go`

**Test coverage could be improved** (common for early-stage products). Priority should be:
1. Domain logic (state machines, business rules) - Partially covered ✓
2. Service layer (business logic orchestration) - Missing tests
3. Handler layer (HTTP concerns) - Minimal coverage
4. Integration tests (end-to-end workflows) - Minimal

### Key Design Patterns Used

1. **Repository Pattern** - Data access abstraction (sqlc-generated)
2. **Service Layer Pattern** - Business logic orchestration
3. **Interface Segregation** - AIProvider, Storage, JobEnqueuer
4. **State Machine** - InspectionStatus, ViolationStatus
5. **Domain Error Types** - Structured error handling
6. **Background Job Queue** - Database-backed worker pattern
7. **Provider Pattern** - AI, Storage, Email implementations
8. **Middleware Chain** - Auth, subscription checks, metrics

### Architecture Decision Records (Implicit)

Based on code analysis, here are the implicit architectural decisions:

1. **Use stdlib router instead of framework** - Simple, no magic, good for small team
2. **sqlc over ORM** - Type-safe queries, no N+1 issues, explicit SQL
3. **Server-side rendering with templ** - Fast, SEO-friendly, simple deployment
4. **Background jobs via database queue** - No external queue needed (Redis, etc.)
5. **Cookie-based sessions** - Simple, works without JavaScript, PostgreSQL-backed
6. **AI provider abstraction** - Can swap providers, mock for testing
7. **Local storage for dev, R2 for prod** - Dev experience, production scalability

All of these are **good decisions** for an early-stage SaaS product.

---

## Conclusion

The Lukaut architecture is **well-designed and ready for alpha launch**. The domain layer is clean, services are properly layered, and infrastructure concerns are abstracted. The main areas for improvement are refinements rather than fundamental issues:

1. Handler-repository coupling for job enqueueing (easily fixable)
2. Service layer coverage for data access (straightforward to complete)
3. Multi-tenancy preparation (defer until validated need)

**Ship the alpha.** The architecture will not be a bottleneck. Address the medium-priority items as you scale to more clients.

The fact that you have:
- Clean domain logic
- Well-defined service interfaces
- Provider abstractions
- State machines for business workflows

...means you have a **solid foundation** to build on. Most architectural problems in early-stage products come from lack of separation of concerns - you've avoided that trap.

**Next steps:**
1. Launch alpha with current architecture ✓
2. After 2-3 clients, fix handler-repository coupling
3. After 5 clients, complete service layer coverage
4. After first request for team features, plan multi-tenancy migration

**Well done.** This is pragmatic, maintainable architecture.
